{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- Robustly set the project root path ---\n",
        "# This allows the notebook to import from other .py files in the project\n",
        "try:\n",
        "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
        "    if project_root not in sys.path:\n",
        "        sys.path.append(project_root)\n",
        "except:\n",
        "    print(\"Could not automatically set project root. Ensure the notebook is in the 'analysis' folder.\")\n",
        "    project_root = '..' # Fallback\n",
        "\n",
        "# --- Import your custom feature engineering functions ---\n",
        "from analysis.feature_engineering import (\n",
        "    download_nltk_resources,\n",
        "    load_mock_data,\n",
        "    calculate_core_linguistic_features,\n",
        "    calculate_catalyst_score\n",
        ")\n",
        "\n",
        "# --- Download NLTK resources (only needs to run once) ---\n",
        "download_nltk_resources()\n",
        "\n",
        "print(\"Setup complete. All modules are loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"--- Part 1: Validating 'Core' Signal vs. Future Volatility ---\")\n",
        "\n",
        "try:\n",
        "    # 1. Load \"Core\" linguistic features from mock filings\n",
        "    core_mock_data = load_mock_data('mock_filings.json', project_root)\n",
        "    core_features_df = calculate_core_linguistic_features(core_mock_data)\n",
        "    core_features_df['date'] = pd.to_datetime(core_features_df['date'])\n",
        "\n",
        "    # 2. Get stock data for the required period\n",
        "    tickers = core_features_df['ticker'].unique().tolist()\n",
        "    start_date = core_features_df['date'].min() - pd.Timedelta(days=1)\n",
        "    # We need 90 days of data *after* the last filing to measure future volatility\n",
        "    end_date = core_features_df['date'].max() + pd.Timedelta(days=91)\n",
        "    \n",
        "    stock_data_df = yf.download(tickers, start=start_date, end=end_date, auto_adjust=True)\n",
        "    stock_data_df = stock_data_df.stack().reset_index()\n",
        "    stock_data_df.rename(columns={'level_1': 'ticker'}, inplace=True)\n",
        "    \n",
        "    # 3. Calculate historical rolling volatility\n",
        "    stock_data_df['returns'] = stock_data_df.groupby('ticker')['Close'].pct_change()\n",
        "    stock_data_df['volatility'] = stock_data_df.groupby('ticker')['returns'].transform(\n",
        "        lambda x: x.rolling(window=30).std() * np.sqrt(252)\n",
        "    )\n",
        "\n",
        "    # 4. Merge linguistic features with FUTURE volatility\n",
        "    merged_data = []\n",
        "    for _, row in core_features_df.iterrows():\n",
        "        call_date = row['date']\n",
        "        future_period = stock_data_df[\n",
        "            (stock_data_df['ticker'] == row['ticker']) &\n",
        "            (stock_data_df['Date'] > call_date) &\n",
        "            (stock_data_df['Date'] <= call_date + pd.Timedelta(days=90))\n",
        "        ]\n",
        "        \n",
        "        if not future_period.empty:\n",
        "            avg_future_volatility = future_period['volatility'].mean()\n",
        "            \n",
        "            new_row = row.to_dict()\n",
        "            new_row['avg_future_volatility'] = avg_future_volatility\n",
        "            merged_data.append(new_row)\n",
        "\n",
        "    final_core_df = pd.DataFrame(merged_data)\n",
        "    \n",
        "    # 5. Correlation Analysis\n",
        "    if not final_core_df.empty:\n",
        "        core_correlation = final_core_df.drop(columns=['ticker', 'date', 'speaker']).corr()\n",
        "        \n",
        "        print(\"\\nCorrelation Matrix for 'Core' Linguistic Features:\")\n",
        "        display(core_correlation[['avg_future_volatility']].style.background_gradient(cmap='viridis'))\n",
        "        \n",
        "    else:\n",
        "        print(\"Could not merge core features with volatility data.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ticker</th>\n",
              "      <th>date</th>\n",
              "      <th>speaker</th>\n",
              "      <th>complexity_score</th>\n",
              "      <th>sentiment_score</th>\n",
              "      <th>generalizing_score</th>\n",
              "      <th>self_reference_score</th>\n",
              "      <th>future_tense_ratio</th>\n",
              "      <th>past_tense_ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ENRN</td>\n",
              "      <td>1999-04-12</td>\n",
              "      <td>CEO</td>\n",
              "      <td>9.242647</td>\n",
              "      <td>0.9601</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.175000</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>0.150000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENRN</td>\n",
              "      <td>1999-04-12</td>\n",
              "      <td>CFO</td>\n",
              "      <td>10.694706</td>\n",
              "      <td>0.9136</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.153846</td>\n",
              "      <td>0.128205</td>\n",
              "      <td>0.128205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ENRN</td>\n",
              "      <td>2000-10-27</td>\n",
              "      <td>CEO</td>\n",
              "      <td>17.305606</td>\n",
              "      <td>0.8225</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ENRN</td>\n",
              "      <td>2000-10-27</td>\n",
              "      <td>CFO</td>\n",
              "      <td>18.166667</td>\n",
              "      <td>0.9100</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>0.120000</td>\n",
              "      <td>0.120000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>2023-01-25</td>\n",
              "      <td>CEO</td>\n",
              "      <td>10.243636</td>\n",
              "      <td>0.9359</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.122449</td>\n",
              "      <td>0.183673</td>\n",
              "      <td>0.183673</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ticker       date speaker  complexity_score  sentiment_score  \\\n",
              "0   ENRN 1999-04-12     CEO          9.242647           0.9601   \n",
              "1   ENRN 1999-04-12     CFO         10.694706           0.9136   \n",
              "2   ENRN 2000-10-27     CEO         17.305606           0.8225   \n",
              "3   ENRN 2000-10-27     CFO         18.166667           0.9100   \n",
              "4   AAPL 2023-01-25     CEO         10.243636           0.9359   \n",
              "\n",
              "   generalizing_score  self_reference_score  future_tense_ratio  \\\n",
              "0               0.025              0.175000            0.150000   \n",
              "1               0.000              0.153846            0.128205   \n",
              "2               0.025              0.050000            0.200000   \n",
              "3               0.040              0.020000            0.120000   \n",
              "4               0.000              0.122449            0.183673   \n",
              "\n",
              "   past_tense_ratio  \n",
              "0          0.150000  \n",
              "1          0.128205  \n",
              "2          0.200000  \n",
              "3          0.120000  \n",
              "4          0.183673  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"\\n--- Part 2: Validating 'Catalyst' Signal via Event Study ---\")\n",
        "\n",
        "try:\n",
        "    # 1. Load \"Catalyst\" event data\n",
        "    catalyst_mock_data = load_mock_data('mock_events.json', project_root)\n",
        "    catalyst_events_df = calculate_catalyst_score(catalyst_mock_data)\n",
        "    catalyst_events_df['date'] = pd.to_datetime(catalyst_events_df['date'])\n",
        "\n",
        "    # 2. Analyze the price impact for each event\n",
        "    event_impacts = []\n",
        "    for _, event in catalyst_events_df.iterrows():\n",
        "        ticker = event['ticker']\n",
        "        event_date = event['date']\n",
        "        \n",
        "        # Define the event window\n",
        "        start_window = event_date - pd.Timedelta(days=30)\n",
        "        end_window = event_date + pd.Timedelta(days=30)\n",
        "        \n",
        "        # Download stock data and benchmark (S&P 500) data\n",
        "        stock_data = yf.download(ticker, start=start_window, end=end_window, auto_adjust=True)['Close']\n",
        "        spy_data = yf.download('SPY', start=start_window, end=end_window, auto_adjust=True)['Close']\n",
        "        \n",
        "        if stock_data.empty:\n",
        "            continue\n",
        "        \n",
        "        # Find the price on the event day and subsequent days\n",
        "        event_day_price = stock_data.asof(event_date)\n",
        "        day5_price = stock_data.asof(event_date + pd.Timedelta(days=5))\n",
        "        day20_price = stock_data.asof(event_date + pd.Timedelta(days=20))\n",
        "        \n",
        "        spy_event_price = spy_data.asof(event_date)\n",
        "        spy_day5_price = spy_data.asof(event_date + pd.Timedelta(days=5))\n",
        "        spy_day20_price = spy_data.asof(event_date + pd.Timedelta(days=20))\n",
        "\n",
        "        # Calculate raw returns and abnormal (market-adjusted) returns\n",
        "        return_5d = (day5_price / event_day_price) - 1\n",
        "        return_20d = (day20_price / event_day_price) - 1\n",
        "        spy_return_5d = (spy_day5_price / spy_event_price) - 1\n",
        "        spy_return_20d = (spy_day20_price / spy_event_price) - 1\n",
        "        \n",
        "        abnormal_return_5d = return_5d - spy_return_5d\n",
        "        abnormal_return_20d = return_20d - spy_return_20d\n",
        "        \n",
        "        impact = event.to_dict()\n",
        "        impact['abnormal_return_5d'] = abnormal_return_5d\n",
        "        impact['abnormal_return_20d'] = abnormal_return_20d\n",
        "        event_impacts.append(impact)\n",
        "\n",
        "    event_impact_df = pd.DataFrame(event_impacts)\n",
        "    \n",
        "    # 3. Display Results\n",
        "    if not event_impact_df.empty:\n",
        "        print(\"\\nPrice Impact Analysis for 'Catalyst' Events:\")\n",
        "        \n",
        "        # Select and format columns for display\n",
        "        display_cols = [\n",
        "            'date', 'ticker', 'event_type', 'source', 'Attack_Score', \n",
        "            'abnormal_return_5d', 'abnormal_return_20d'\n",
        "        ]\n",
        "        # Handle cases where a column might not exist (e.g., Rebuttal_Score)\n",
        "        if 'Rebuttal_Severity_Score' in event_impact_df.columns:\n",
        "            display_cols.insert(5, 'Rebuttal_Severity_Score')\n",
        "        \n",
        "        # Filter for columns that actually exist in the dataframe\n",
        "        display_cols = [col for col in display_cols if col in event_impact_df.columns]\n",
        "        \n",
        "        formatted_df = event_impact_df[display_cols].copy()\n",
        "        formatted_df['abnormal_return_5d'] = formatted_df['abnormal_return_5d'].map('{:.2%}'.format)\n",
        "        formatted_df['abnormal_return_20d'] = formatted_df['abnormal_return_20d'].map('{:.2%}'.format)\n",
        "\n",
        "        display(formatted_df)\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(e)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
